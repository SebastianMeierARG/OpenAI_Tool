{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and loading pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\semeier\\Desktop\\AI Project\\GT_AI_Project\\gemini_chat-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "import fitz  # PyMuPDF\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "\n",
    "# create function that gets config.yaml parameters\n",
    "def import_config_params(config_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Load configuration from config.yaml located in the project folder.\n",
    "    Returns a dict with configuration (empty dict if not found).\n",
    "    \"\"\"\n",
    "    if config_path is None:\n",
    "        config_path = os.path.join(os.path.dirname(__file__), \"config.yaml\")\n",
    "    try:\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f) or {}\n",
    "            return cfg\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[config] Warning: config.yaml not found at {config_path}. Using defaults.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"[config] Error loading config.yaml: {e}. Using defaults.\")\n",
    "        return {}\n",
    "    \n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not found in .env\")\n",
    "\n",
    "# load YAML configuration and derive runtime parameters\n",
    "config = import_config_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 PDFs from ./pdf_files/ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding PDF chunks ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:06<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF index built with 236 chunks. Embedding dim=384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model / embedder\n",
    "MODEL_NAME = config.get(\"model\", {}).get(\"name\", \"gemini-2.5-flash\")\n",
    "EMBEDDER_MODEL = config.get(\"embedder_model\", \"all-MiniLM-L6-v2\")\n",
    "\n",
    "# pdf + chunking settings\n",
    "PDF_FOLDER = config.get(\"pdf\", {}).get(\"folder\", \"./pdf_files/\")\n",
    "CHUNK_SIZE = config.get(\"chunking\", {}).get(\"chunk_size\", 1100)\n",
    "CHUNK_OVERLAP = config.get(\"chunking\", {}).get(\"overlap\", 150)\n",
    "\n",
    "# serpapi defaults\n",
    "SERPAPI_DEFAULTS = config.get(\"serpapi\", {\"k\": 8, \"hl\": \"en\", \"gl\": \"es\", \"last_year_only\": True})\n",
    "\n",
    "# retrieval\n",
    "RETRIEVAL = config.get(\"retrieval\", {\"top_k_pdf\": 5, \"top_k_web\": 4, \"max_chars\": 2400})\n",
    "TOP_K_PDF = RETRIEVAL.get(\"top_k_pdf\", 5)\n",
    "TOP_K_WEB = RETRIEVAL.get(\"top_k_web\", 4)\n",
    "MAX_CHARS = RETRIEVAL.get(\"max_chars\", 2400)\n",
    "\n",
    "SYSTEM_INSTRUCTIONS = config.get(\"system_instructions\", (\n",
    "    \"You are a credit risk and regulation assistant. \"\n",
    "    \"Use the provided context (PDF and web snippets) when relevant, \"\n",
    "    \"and you may also use your own knowledge. \"\n",
    "    \"When you use web snippets, include a 'Sources:' section with URLs.\"\n",
    "))\n",
    "\n",
    "GRADIO_CFG = config.get(\"gradio\", {\"server_name\": \"127.0.0.1\", \"server_port\": 7860, \"share\": False})\n",
    "\n",
    "# configure Gemini + embedder\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "embedder = SentenceTransformer(EMBEDDER_MODEL)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "# -------------------- PDF Ingestion --------------------\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1100, overlap: int = 150):\n",
    "    chunks, i = [], 0\n",
    "    while i < len(text):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "pdf_folder = PDF_FOLDER\n",
    "if not os.path.isdir(pdf_folder):\n",
    "    os.makedirs(pdf_folder, exist_ok=True)\n",
    "\n",
    "documents, doc_sources = [], []\n",
    "pdf_chunks, pdf_chunk_meta = [], []\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "for file in pdf_files:\n",
    "    file_path = os.path.join(pdf_folder, file)\n",
    "    raw = extract_text_from_pdf(file_path)\n",
    "    raw = clean_text(raw)\n",
    "    documents.append(raw)\n",
    "    doc_sources.append(file)\n",
    "\n",
    "    chunks = chunk_text(raw, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "    for ci, c in enumerate(chunks):\n",
    "        pdf_chunks.append(c)\n",
    "        pdf_chunk_meta.append({\"source\": file, \"chunk_id\": ci})\n",
    "\n",
    "if pdf_chunks:\n",
    "    pdf_embeddings = embedder.encode(pdf_chunks, convert_to_numpy=True, show_progress_bar=False)\n",
    "    dimension = pdf_embeddings.shape[1]\n",
    "    index_pdf = faiss.IndexFlatL2(dimension)\n",
    "    index_pdf.add(pdf_embeddings.astype(np.float32))\n",
    "else:\n",
    "    pdf_embeddings = None\n",
    "    index_pdf = None\n",
    "    dimension = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Optional: rouge scorer if available\n",
    "try:\n",
    "    _rouge_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "except Exception:\n",
    "    _rouge_scorer = None\n",
    "\n",
    "# NOTE: This cell relies on existing names from the notebook:\n",
    "# - query_hybrid(prompt) -> str\n",
    "# - embedder -> SentenceTransformer instance\n",
    "# If you want to evaluate already-collected responses, set call_model=False\n",
    "# and provide 'response' in each dataset entry.\n",
    "\n",
    "def _extract_urls(text: str) -> List[str]:\n",
    "    return re.findall(r\"https?://\\S+\", text)\n",
    "\n",
    "def _has_sources_section(text: str) -> bool:\n",
    "    return bool(re.search(r\"\\bSources:\\b\", text, flags=re.IGNORECASE)) or bool(_extract_urls(text))\n",
    "\n",
    "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-12\n",
    "    nb = np.linalg.norm(b) + 1e-12\n",
    "    return float((a @ b.T) / (na * nb))\n",
    "\n",
    "def evaluate_single(prompt: str,\n",
    "                    reference: Optional[str] = None,\n",
    "                    call_model: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Run the genAI model (unless call_model is False) and compute automatic metrics.\n",
    "    Returns a dict with response, timing, cosine similarity (embedding), rouge-L (if available),\n",
    "    source detection and simple length stats.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    response = query_hybrid(prompt) if call_model else \"\"\n",
    "    latency = time.time() - start\n",
    "\n",
    "    # embeddings-based similarity (requires a reference)\n",
    "    cos_sim = None\n",
    "    if reference:\n",
    "        resp_emb = embedder.encode([response], convert_to_numpy=True)\n",
    "        ref_emb = embedder.encode([reference], convert_to_numpy=True)\n",
    "        cos_sim = _cosine_similarity(resp_emb[0], ref_emb[0])\n",
    "\n",
    "    # rouge-l (optional)\n",
    "    rouge_l_f = None\n",
    "    if reference and _rouge_scorer is not None:\n",
    "        scr = _rouge_scorer.score(reference, response)\n",
    "        rouge_l_f = scr[\"rougeL\"].fmeasure\n",
    "\n",
    "    urls = _extract_urls(response)\n",
    "    has_sources = _has_sources_section(response)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"reference\": reference,\n",
    "        \"latency_s\": latency,\n",
    "        \"len_response_chars\": len(response),\n",
    "        \"num_urls\": len(urls),\n",
    "        \"has_sources_section\": has_sources,\n",
    "        \"cosine_sim\": cos_sim,\n",
    "        \"rougeL_f\": rouge_l_f,\n",
    "        \"urls\": urls,\n",
    "    }\n",
    "\n",
    "def evaluate_dataset(dataset: List[Dict],\n",
    "                     call_model: bool = True,\n",
    "                     show_progress: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dataset: list of {\"prompt\": str, \"reference\": Optional[str], \"response\": Optional[str]}\n",
    "    If call_model=True, 'response' is ignored and query_hybrid is invoked for each prompt.\n",
    "    If call_model=False, 'response' must be present in each entry.\n",
    "    Returns a pandas DataFrame with metrics per item and prints a short summary.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if show_progress:\n",
    "            print(f\"Evaluating {i+1}/{len(dataset)}\", end=\"\\r\")\n",
    "        prompt = item.get(\"prompt\")\n",
    "        reference = item.get(\"reference\")\n",
    "        if not call_model and \"response\" in item:\n",
    "            # Use provided response\n",
    "            resp = item[\"response\"]\n",
    "            # We create a tiny wrapper to avoid calling model\n",
    "            def _dummy_query(_p): return resp\n",
    "            global query_hybrid  # temporarily swap\n",
    "            _orig = query_hybrid\n",
    "            query_hybrid = _dummy_query\n",
    "            try:\n",
    "                r = evaluate_single(prompt, reference, call_model=True)\n",
    "            finally:\n",
    "                query_hybrid = _orig\n",
    "        else:\n",
    "            r = evaluate_single(prompt, reference, call_model=call_model)\n",
    "        results.append(r)\n",
    "    df = pd.DataFrame(results)\n",
    "    # summary\n",
    "    avg_cos = df[\"cosine_sim\"].dropna().mean() if \"cosine_sim\" in df else None\n",
    "    avg_rouge = df[\"rougeL_f\"].dropna().mean() if \"rougeL_f\" in df else None\n",
    "    pct_with_sources = 100.0 * df[\"has_sources_section\"].mean() if not df.empty else 0.0\n",
    "    print(\"\\n--- Evaluation summary ---\")\n",
    "    print(f\"Items: {len(df)}\")\n",
    "    if avg_cos is not None:\n",
    "        print(f\"Avg embedding cosine similarity (resp vs ref): {avg_cos:.4f}\")\n",
    "    if avg_rouge is not None:\n",
    "        print(f\"Avg ROUGE-L F1: {avg_rouge:.4f}\")\n",
    "    print(f\"% responses including a 'Sources' section or URLs: {pct_with_sources:.1f}%\")\n",
    "    print(f\"Avg latency (s): {df['latency_s'].mean():.2f}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- SerpAPI Search --------------------\n",
    "def web_search_serpapi(query: str, k: int = None, hl: str = None, gl: str = None, last_year_only: bool = None):\n",
    "    if not SERPAPI_API_KEY:\n",
    "        return []\n",
    "\n",
    "    k = k if k is not None else SERPAPI_DEFAULTS.get(\"k\", 8)\n",
    "    hl = hl if hl is not None else SERPAPI_DEFAULTS.get(\"hl\", \"en\")\n",
    "    gl = gl if gl is not None else SERPAPI_DEFAULTS.get(\"gl\", \"es\")\n",
    "    last_year_only = last_year_only if last_year_only is not None else SERPAPI_DEFAULTS.get(\"last_year_only\", True)\n",
    "\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_API_KEY,\n",
    "        \"num\": k,\n",
    "        \"hl\": hl,\n",
    "        \"gl\": gl,\n",
    "        \"safe\": \"active\",\n",
    "        \"filter\": \"1\",\n",
    "    }\n",
    "    if last_year_only:\n",
    "        params[\"tbs\"] = \"qdr:y\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(\"https://serpapi.com/search.json\", params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        org = data.get(\"organic_results\", []) or []\n",
    "        cleaned = []\n",
    "        for item in org:\n",
    "            title = item.get(\"title\", \"\")\n",
    "            link = item.get(\"link\", \"\") or item.get(\"url\", \"\")\n",
    "            snippet = item.get(\"snippet\", \"\") or item.get(\"content\", \"\")\n",
    "            if not link:\n",
    "                continue\n",
    "            cleaned.append({\n",
    "                \"title\": title,\n",
    "                \"url\": link,\n",
    "                \"content\": clean_text(snippet)[:2000]\n",
    "            })\n",
    "        return cleaned\n",
    "    except Exception as e:\n",
    "        print(f\"[web] SerpAPI error: {e}\")\n",
    "        return []\n",
    "\n",
    "def embed_texts(texts: List[str]):\n",
    "    return embedder.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "# -------------------- Retrieval + Generation --------------------\n",
    "def retrieve_hybrid(query: str, top_k_pdf: int = None, top_k_web: int = None):\n",
    "    top_k_pdf = top_k_pdf if top_k_pdf is not None else TOP_K_PDF\n",
    "    top_k_web = top_k_web if top_k_web is not None else TOP_K_WEB\n",
    "\n",
    "    pdf_hits = []\n",
    "    if index_pdf is not None and pdf_chunks:\n",
    "        q_emb = embed_texts([query]).astype(np.float32)\n",
    "        distances, indices = index_pdf.search(q_emb, min(top_k_pdf, len(pdf_chunks)))\n",
    "        for rank, idx in enumerate(indices[0]):\n",
    "            score = -float(distances[0][rank])\n",
    "            pdf_hits.append({\n",
    "                \"text\": pdf_chunks[idx],\n",
    "                \"score\": score,\n",
    "                \"source\": pdf_chunk_meta[idx][\"source\"],\n",
    "                \"chunk_id\": pdf_chunk_meta[idx][\"chunk_id\"]\n",
    "            })\n",
    "\n",
    "    web_hits = []\n",
    "    web_results = web_search_serpapi(query, k=8) if top_k_web > 0 else []\n",
    "    if web_results:\n",
    "        web_texts = [w[\"content\"] for w in web_results if w.get(\"content\")]\n",
    "        if web_texts:\n",
    "            q_emb = embed_texts([query]).astype(np.float32)\n",
    "            w_embs = embed_texts(web_texts).astype(np.float32)\n",
    "            qn = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)\n",
    "            wn = w_embs / (np.linalg.norm(w_embs, axis=1, keepdims=True) + 1e-12)\n",
    "            sims = (wn @ qn[0])\n",
    "            top_idx = np.argsort(-sims)[:top_k_web]\n",
    "            for i in top_idx:\n",
    "                w = web_results[i]\n",
    "                web_hits.append({\n",
    "                    \"text\": w[\"content\"],\n",
    "                    \"score\": float(sims[i]),\n",
    "                    \"url\": w[\"url\"],\n",
    "                    \"title\": w.get(\"title\", \"\")\n",
    "                })\n",
    "    return pdf_hits, web_hits\n",
    "\n",
    "def build_context_and_citations(pdf_hits, web_hits, max_chars: int = 2400):\n",
    "    combined = []\n",
    "    for i in range(max(len(pdf_hits), len(web_hits))):\n",
    "        if i < len(pdf_hits): combined.append((\"pdf\", pdf_hits[i]))\n",
    "        if i < len(web_hits): combined.append((\"web\", web_hits[i]))\n",
    "\n",
    "    context_parts, citations, used = [], [], 0\n",
    "    for typ, item in combined:\n",
    "        snippet = item[\"text\"].strip()\n",
    "        if not snippet:\n",
    "            continue\n",
    "        header = f\"[PDF:{item['source']}#chunk{item['chunk_id']}]\" if typ == \"pdf\" else f\"[WEB:{item.get('title','')}]\"\n",
    "        block = f\"{header}\\n{snippet}\\n\"\n",
    "        if used + len(block) > max_chars:\n",
    "            break\n",
    "        context_parts.append(block)\n",
    "        used += len(block)\n",
    "        if typ == \"web\" and item.get(\"url\"):\n",
    "            citations.append(item[\"url\"])\n",
    "    return \"\\n\\n---\\n\\n\".join(context_parts), citations\n",
    "\n",
    "## Please add to  config.yaml file with all the parameters for the model configuration (web_search_serpapi params, SYSTEM_INSTRUCTIONS, etc.\n",
    "def answer_query(user_input: str):\n",
    "    if not user_input or not user_input.strip():\n",
    "        return \"\", \"Please enter a question.\"\n",
    "    pdf_hits, web_hits = retrieve_hybrid(user_input, top_k_pdf=5, top_k_web=4)\n",
    "    context, urls = build_context_and_citations(pdf_hits, web_hits, max_chars=2400)\n",
    "\n",
    "    final_prompt = (\n",
    "        f\"{SYSTEM_INSTRUCTIONS}\\n\\n\"\n",
    "        f\"User question: {user_input}\\n\\n\"\n",
    "        f\"Context below. Prefer it for factual grounding.\\n\"\n",
    "        f\"{'-'*40}\\n{context}\\n{'-'*40}\\n\\n\"\n",
    "        \"If you used any web evidence, add a short 'Sources:' list of URLs at the end.\"\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(final_prompt)\n",
    "    text = (response.text or \"\").strip()\n",
    "    if urls:\n",
    "        uniq_urls = list(dict.fromkeys(urls))\n",
    "        text += \"\\n\\nSources:\\n\" + \"\\n\".join(uniq_urls)\n",
    "    return text, \", \".join(dict.fromkeys(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Gradio UI --------------------\n",
    "with gr.Blocks(title=\"Capitalflow Hybrid RAG\") as demo:\n",
    "    gr.Markdown(\"## ðŸ¤– Capitalflow Hybrid RAG (PDFs + Web via SerpAPI + Gemini 2.5 Flash)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        query = gr.Textbox(label=\"Ask a question\", placeholder=\"e.g., Summarize PD calculation approaches...\")\n",
    "    with gr.Row():\n",
    "        output = gr.Markdown(label=\"Answer\")\n",
    "    sources = gr.Textbox(label=\"Sources (URLs)\", interactive=False)\n",
    "\n",
    "    submit = gr.Button(\"Ask\")\n",
    "    submit.click(fn=answer_query, inputs=[query], outputs=[output, sources])\n",
    "\n",
    "\n",
    "demo.launch(\n",
    "    server_name=GRADIO_CFG.get(\"server_name\", \"127.0.0.1\"),\n",
    "    server_port=GRADIO_CFG.get(\"server_port\", 7860),\n",
    "    share=bool(GRADIO_CFG.get(\"share\", False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: in few words, How do EBA manage low Default portfolios in the context of the PD?\n",
      "EBA applies a proportionate approach to Probability of Default (PD) estimation, considering factors such as portfolio complexity and the availability of data. This allows for adaptability when information is scarce, as is often the case with low default portfolios.\n",
      "\n",
      "Sources:\n",
      "*   [WEB:Consultation paper - European Banking Authority](https://www.eba.europa.eu/consultations/eba-consultation-paper-draft-guidelines-probability-default-pd-estimation)\n",
      "\n",
      "Sources:\n",
      "https://www.eba.europa.eu/sites/default/files/2025-07/b3f9af47-ab61-4e89-94f2-7910c39c372f/Consultation%20paper%20Guidelines%20CCF.pdf\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = \"in few words, How do EBA manage low Default portfolios in the context of the PD?\"\n",
    "print(\"Q:\", q1)\n",
    "print(query_hybrid(q1))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 1/1\n",
      "--- Evaluation summary ---\n",
      "Items: 1\n",
      "Avg embedding cosine similarity (resp vs ref): 0.7328\n",
      "Avg ROUGE-L F1: 0.1734\n",
      "% responses including a 'Sources' section or URLs: 100.0%\n",
      "Avg latency (s): 31.21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>len_response_chars</th>\n",
       "      <th>num_urls</th>\n",
       "      <th>has_sources_section</th>\n",
       "      <th>cosine_sim</th>\n",
       "      <th>rougeL_f</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in few words, How do EBA manage low Default po...</td>\n",
       "      <td>The EBA manages low default portfolios in the ...</td>\n",
       "      <td>The EBA mandates that institutions compensate ...</td>\n",
       "      <td>31.211411</td>\n",
       "      <td>909</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.732788</td>\n",
       "      <td>0.17341</td>\n",
       "      <td>[https://www.eba.europa.eu/regulation-and-poli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  in few words, How do EBA manage low Default po...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The EBA manages low default portfolios in the ...   \n",
       "\n",
       "                                           reference  latency_s  \\\n",
       "0  The EBA mandates that institutions compensate ...  31.211411   \n",
       "\n",
       "   len_response_chars  num_urls  has_sources_section  cosine_sim  rougeL_f  \\\n",
       "0                 909         2                 True    0.732788   0.17341   \n",
       "\n",
       "                                                urls  \n",
       "0  [https://www.eba.europa.eu/regulation-and-poli...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Provide references (gold answers) when available.\n",
    "dataset = [\n",
    "    {\"prompt\": q1, \"reference\": \"The EBA mandates that institutions compensate for data scarcity in Low Default Portfolios by relying on external data pooling, shadow rating models, and strict margins of conservatism. This ensures that PD estimates remain prudent and non-zero, even in the absence of historical internal defaults.\"\n",
    "    #{\"prompt\": q2, \"reference\": \"<short gold summary of PD calculation approaches>\"},\n",
    "\n",
    "    }\n",
    "]\n",
    "df_metrics = evaluate_dataset(dataset, call_model=True)\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gemini Project)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
